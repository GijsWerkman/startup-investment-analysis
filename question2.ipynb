{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e5d440-4f6b-4003-9772-00246e3a3c2b",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "### 1.1 Problem definition\n",
    "In this project, our primary goal was to construct a predictive model capable of estimating the total funding amount in USD that startups receive, making 'funding_total_usd' our target variable. \n",
    "\n",
    "### 1.2 Overview of models\n",
    "To tackle this predictive challenge, we employed a variety of machine learning models, each with distinct characteristics and complexity levels:\n",
    "- K-Nearest Neighbors (K-NN): A simple, intuitive model that makes predictions based on the 'closeness' of data points in a feature space.\n",
    "- Random Forest: An ensemble approach that leverages multiple decision trees to enhance prediction accuracy and robustness, effectively reducing overfitting.\n",
    "- LightGBM: A gradient boosting framework that uses tree-based learning algorithms, known for its efficiency and effectiveness, particularly on large datasets.\n",
    "- Lasso Regression: A linear model with regularization to prevent overfitting, especially useful when the dataset has a high dimensionality due to many encoded categorical features.\n",
    "\n",
    "### 1.3 Feature engineering\n",
    "Prior to the modelling, we had already conducted extensive data preprocessing and feature engineering. Nevertheless, some models required further modifications to tailor to their specific requirements:\n",
    "- Data cleaning: Removed irrelevant columns and handled missing values to prepare a clean dataset.\n",
    "- Feature engineering: Calculated the duration between different funding rounds to potentially capture the momentum of funding interest.\n",
    "- Categorical encoding: Transformed categorical variables using binary encoding to prepare them for machine learning models, allowing models to better understand the patterns in categorical data.\n",
    "- Feature selection: Identified and dropped features that did not contribute to the predictive power of the model, focusing on those most relevant to the target variable.\n",
    "\n",
    "### 1.4 Evaluation metrics\n",
    "For the evaluation of our models, we chose R² as the primary metric due to its clarity and independence from scale. R² measures the proportion of variance in the dependent variable that can be predicted from the independent variables, providing a standardized approach that remains unaffected by the magnitude of the funding amounts or the complexity introduced by numerous predictors. This characteristic allowed for straightforward comparisons across different modeling approaches, enhancing our understanding of each model's explanatory power.\r\n",
    "\r\n",
    "While R² served as our metric of choice, we did consider alternative metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). These metrics provide valuable insights into the magnitude of prediction errors and the models' robustness to outliers. However, we ultimately favored R² for its ability to measure performance in a relative context, which was particularly aligned with the objectives of our project. This decision was based on the importance of quantifying how effectively our models could explain variations in funding amounts, rather than just the scale of their errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170e947-51ee-4931-ae40-4a199b39108f",
   "metadata": {},
   "source": [
    "# 2. K-Nearest Neighbors\n",
    "### 2.1 Model introduction\n",
    "Knn is a simple, non-parametric algorithm used for both classification and regression tasks. In regression, like in this case, it predicts the output variable by averaging the values of its k nearest neighbours' target varlues. The \"nearest\" neighbours are determined based on a distance metric. In this case we decided to use the Euclidean distance. We started off with KNN as it is a simple model and provides a baseline comparison to the rest of the models. The simplicity makes the model relatively intuitive and easy to understand. Another advantage is that KNN is able to capture complex patterns in the data without imposing rigid assumptions. \n",
    "\n",
    "The following code will first load the necessary libraries and data afterwhich the data is preprocessed. The data was already preprocessed by the team yet some further adjustments were made below namely, the creation of a new variable (funding_duration), dropping of some columns that became redundant and standardizing several variable names. Secondly, the variables categorical variables were encoded using binary encoding. Then the data was split in a training and test set in order to validate the model predictions. To capture most of the training data, the team used cross fold validation. Lastly, the model was defined, trained and tested. The model evaluation was done based on the R². "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1656b-2e72-467f-b696-200dad7d1479",
   "metadata": {},
   "source": [
    "### 2.2 Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9b3b0a-edef-40cf-9300-ac11455f7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45af85-e4eb-4e0a-8811-39204eefc1c0",
   "metadata": {},
   "source": [
    "### 2.3 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34bb190a-dd1a-4158-8c1f-4eeb951b49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       permalink  Sequoia Capital  Kleiner Perkins Caufield & Byers  \\\n",
      "0              0            False                             False   \n",
      "1              1            False                             False   \n",
      "2              2            False                             False   \n",
      "3              3            False                             False   \n",
      "4              4            False                             False   \n",
      "...          ...              ...                               ...   \n",
      "28714      28714            False                             False   \n",
      "28715      28715            False                             False   \n",
      "28716      28716            False                             False   \n",
      "28717      28717            False                             False   \n",
      "28718      28718            False                             False   \n",
      "\n",
      "       New Enterprise Associates  Accel Partners  Intel Capital  \\\n",
      "0                          False           False          False   \n",
      "1                          False           False          False   \n",
      "2                          False           False          False   \n",
      "3                          False           False          False   \n",
      "4                          False           False          False   \n",
      "...                          ...             ...            ...   \n",
      "28714                      False           False          False   \n",
      "28715                      False           False          False   \n",
      "28716                      False           False          False   \n",
      "28717                      False           False          False   \n",
      "28718                      False           False           True   \n",
      "\n",
      "       Draper Fisher Jurvetson (DFJ)  First Round  SV Angel  500 Startups  \\\n",
      "0                              False         True     False         False   \n",
      "1                              False        False     False         False   \n",
      "2                              False        False     False         False   \n",
      "3                              False        False     False         False   \n",
      "4                              False        False     False         False   \n",
      "...                              ...          ...       ...           ...   \n",
      "28714                          False        False     False         False   \n",
      "28715                          False        False     False         False   \n",
      "28716                          False        False     False         False   \n",
      "28717                          False        False     False         False   \n",
      "28718                          False        False     False         False   \n",
      "\n",
      "       ...  last_founded_year  last_founded_month  last_founded_dayofyear  \\\n",
      "0      ...               2012                   6                     182   \n",
      "1      ...               2012                   8                     222   \n",
      "2      ...               2011                   4                      91   \n",
      "3      ...               2014                   9                     269   \n",
      "4      ...               2013                   5                     151   \n",
      "...    ...                ...                 ...                     ...   \n",
      "28714  ...               2013                   2                      46   \n",
      "28715  ...               2013                   1                      29   \n",
      "28716  ...               2014                   3                      83   \n",
      "28717  ...               2014                   9                     253   \n",
      "28718  ...               2013                   4                      94   \n",
      "\n",
      "       acquired_year_new  acquired_month_new  acquired_dayofyear  \\\n",
      "0                 2013.0                10.0               290.0   \n",
      "1                    NaN                 NaN                 NaN   \n",
      "2                    NaN                 NaN                 NaN   \n",
      "3                    NaN                 NaN                 NaN   \n",
      "4                    NaN                 NaN                 NaN   \n",
      "...                  ...                 ...                 ...   \n",
      "28714                NaN                 NaN                 NaN   \n",
      "28715                NaN                 NaN                 NaN   \n",
      "28716                NaN                 NaN                 NaN   \n",
      "28717                NaN                 NaN                 NaN   \n",
      "28718                NaN                 NaN                 NaN   \n",
      "\n",
      "       funded_year_new  funded_month_new  funded_dayofyear  investor_count  \n",
      "0                 2012                 6               182               6  \n",
      "1                 2012                 8               222               1  \n",
      "2                 2011                 4                91               1  \n",
      "3                 2014                 8               229               1  \n",
      "4                 2013                 5               151               1  \n",
      "...                ...               ...               ...             ...  \n",
      "28714             2013                 2                46               4  \n",
      "28715             2013                 1                29               1  \n",
      "28716             2014                 3                83               2  \n",
      "28717             2014                 9               253               1  \n",
      "28718             2013                 4                94               5  \n",
      "\n",
      "[28719 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "startup_df = pd.read_csv('cleaned.csv')\n",
    "\n",
    "print(startup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2548a6-a70b-4030-a7cd-ae0ca19fd742",
   "metadata": {},
   "source": [
    "### 2.4 Data engineering\n",
    "In this step, some feature engineering steps were required to make the data machine processible and improve model accuracy. The first code block creates a new variable that shows the duration between the date the first and the last funds were raised. This represents the duration within a funding round was finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021e2806-821b-4318-94b1-7f463cfbb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable reflecting the duration between the raised amount and the last funded\n",
    "startup_df['funded_at'] = pd.to_datetime(startup_df['funded_at'])\n",
    "startup_df['last_funding_at'] = pd.to_datetime(startup_df['last_funding_at'])\n",
    "\n",
    "# Calculate the funding duration in days\n",
    "startup_df['funding_duration'] = (startup_df['last_funding_at'] - startup_df['funded_at']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad8485-ceae-4588-ad87-ff0c93e5c0e2",
   "metadata": {},
   "source": [
    "Then the dataset still contains some variables that are not machine processible due to its datatype. The below variables are date indicators yet as the team created new variables in the data cleaning process these variables have become redundant. Therefore, the team decided to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a56c2cc-e98e-4997-b4cc-e7b658066d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['founded_at', 'first_funding_at', 'last_funding_at', 'acquired_at', 'funded_at']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "startup_df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dca261-6e72-4bcb-b913-aab04a1cb01c",
   "metadata": {},
   "source": [
    "In the data cleaning process, the variable investor_name was transformed into different dummy variables of the 50 most frequent investors. These dummy variables still contain some characters that are not machine processible. All these characters were replaced with an underscore (_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e339d56f-8946-4ac5-81ed-dac5e533ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing 'Sequoia Capital' with 'Sequoia_Capital'\n",
      "Replacing 'Kleiner Perkins Caufield & Byers' with 'Kleiner_Perkins_Caufield_&_Byers'\n",
      "Replacing 'New Enterprise Associates' with 'New_Enterprise_Associates'\n",
      "Replacing 'Accel Partners' with 'Accel_Partners'\n",
      "Replacing 'Intel Capital' with 'Intel_Capital'\n",
      "Replacing 'Draper Fisher Jurvetson (DFJ)' with 'Draper_Fisher_Jurvetson_(DFJ)'\n",
      "Replacing 'First Round' with 'First_Round'\n",
      "Replacing 'SV Angel' with 'SV_Angel'\n",
      "Replacing '500 Startups' with '500_Startups'\n",
      "Replacing 'Bessemer Venture Partners' with 'Bessemer_Venture_Partners'\n",
      "Replacing 'Greylock Partners' with 'Greylock_Partners'\n",
      "Replacing 'Lightspeed Venture Partners' with 'Lightspeed_Venture_Partners'\n",
      "Replacing 'Andreessen Horowitz' with 'Andreessen_Horowitz'\n",
      "Replacing 'Khosla Ventures' with 'Khosla_Ventures'\n",
      "Replacing 'Index Ventures' with 'Index_Ventures'\n",
      "Replacing 'Canaan Partners' with 'Canaan_Partners'\n",
      "Replacing 'Redpoint Ventures' with 'Redpoint_Ventures'\n",
      "Replacing 'Norwest Venture Partners - NVP' with 'Norwest_Venture_Partners_-_NVP'\n",
      "Replacing 'Menlo Ventures' with 'Menlo_Ventures'\n",
      "Replacing 'General Catalyst Partners' with 'General_Catalyst_Partners'\n",
      "Replacing 'U.S. Venture Partners' with 'U_S__Venture_Partners'\n",
      "Replacing 'Sigma Partners' with 'Sigma_Partners'\n",
      "Replacing 'Battery Ventures' with 'Battery_Ventures'\n",
      "Replacing 'DAG Ventures' with 'DAG_Ventures'\n",
      "Replacing 'Foundation Capital' with 'Foundation_Capital'\n",
      "Replacing 'Google Ventures' with 'Google_Ventures'\n",
      "Replacing 'Atlas Venture' with 'Atlas_Venture'\n",
      "Replacing 'Highland Capital Partners' with 'Highland_Capital_Partners'\n",
      "Replacing 'Polaris Partners' with 'Polaris_Partners'\n",
      "Replacing 'Mayfield Fund' with 'Mayfield_Fund'\n",
      "Replacing 'Silicon Valley Bank' with 'Silicon_Valley_Bank'\n",
      "Replacing 'Y Combinator' with 'Y_Combinator'\n",
      "Replacing 'Matrix Partners' with 'Matrix_Partners'\n",
      "Replacing 'True Ventures' with 'True_Ventures'\n",
      "Replacing 'Mohr Davidow Ventures' with 'Mohr_Davidow_Ventures'\n",
      "Replacing 'North Bridge Venture Partners' with 'North_Bridge_Venture_Partners'\n",
      "Replacing 'InterWest Partners' with 'InterWest_Partners'\n",
      "Replacing 'RRE Ventures' with 'RRE_Ventures'\n",
      "Replacing 'Oak Investment Partners' with 'Oak_Investment_Partners'\n",
      "Replacing 'Felicis Ventures' with 'Felicis_Ventures'\n",
      "Replacing 'Shasta Ventures' with 'Shasta_Ventures'\n",
      "Replacing 'Start-Up Chile' with 'Start-Up_Chile'\n",
      "Replacing 'Trinity Ventures' with 'Trinity_Ventures'\n",
      "Replacing 'Goldman Sachs' with 'Goldman_Sachs'\n",
      "Replacing 'Union Square Ventures' with 'Union_Square_Ventures'\n",
      "Modified columns: Index(['permalink', 'Sequoia_Capital', 'Kleiner_Perkins_Caufield_&_Byers',\n",
      "       'New_Enterprise_Associates', 'Accel_Partners', 'Intel_Capital',\n",
      "       'Draper_Fisher_Jurvetson_(DFJ)', 'First_Round', 'SV_Angel',\n",
      "       '500_Startups', 'Bessemer_Venture_Partners', 'Greylock_Partners',\n",
      "       'Benchmark', 'Lightspeed_Venture_Partners', 'Andreessen_Horowitz',\n",
      "       'Khosla_Ventures', 'Index_Ventures', 'Canaan_Partners',\n",
      "       'Redpoint_Ventures', 'Venrock', 'Norwest_Venture_Partners_-_NVP',\n",
      "       'Menlo_Ventures', 'General_Catalyst_Partners', 'U_S__Venture_Partners',\n",
      "       'Sigma_Partners', 'Battery_Ventures', 'DAG_Ventures',\n",
      "       'Foundation_Capital', 'Google_Ventures', 'Atlas_Venture',\n",
      "       'Highland_Capital_Partners', 'Techstars', 'Polaris_Partners',\n",
      "       'Mayfield_Fund', 'Silicon_Valley_Bank', 'Y_Combinator',\n",
      "       'Matrix_Partners', 'True_Ventures', 'Mohr_Davidow_Ventures',\n",
      "       'North_Bridge_Venture_Partners', 'InterWest_Partners', 'RRE_Ventures',\n",
      "       'Oak_Investment_Partners', 'Felicis_Ventures', 'Shasta_Ventures',\n",
      "       'Start-Up_Chile', 'Trinity_Ventures', 'CRV', 'Goldman_Sachs',\n",
      "       'Union_Square_Ventures', 'DCM', 'category_list', 'market',\n",
      "       'funding_total_usd', 'status', 'country_code', 'state_code', 'region',\n",
      "       'funding_rounds', 'acquirer_permalink', 'acquirer_category_list',\n",
      "       'acquirer_market', 'acquirer_country_code', 'acquirer_state_code',\n",
      "       'acquirer_region', 'price_amount', 'funding_round_permalink',\n",
      "       'funding_round_type', 'funding_round_code', 'raised_amount_usd',\n",
      "       'investor_category_list', 'investor_country_code', 'investor_permalink',\n",
      "       'investor_state_code', 'investor_market', 'investor_region',\n",
      "       'gdp_capita', 'foreign_direct_investment', 'gdp_growth',\n",
      "       'interest_rate', 'female_participation', 'founded_year',\n",
      "       'founded_month', 'founded_dayofyear', 'first_founded_year',\n",
      "       'first_founded_month', 'first_founded_dayofyear', 'last_founded_year',\n",
      "       'last_founded_month', 'last_founded_dayofyear', 'acquired_year_new',\n",
      "       'acquired_month_new', 'acquired_dayofyear', 'funded_year_new',\n",
      "       'funded_month_new', 'funded_dayofyear', 'investor_count',\n",
      "       'funding_duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def check_and_replace_column_names(df):\n",
    "    replaced_columns = []\n",
    "    for column_name in df.columns:\n",
    "        if any(c in column_name for c in [' ', ',', ':', '.', '[', ']', '{', '}', '\"']):\n",
    "            # Replace non-compliant characters with underscores\n",
    "            new_column_name = column_name.replace(' ', '_').replace(',', '_').replace(':', '_').replace('.', '_').replace('[', '_').replace(']', '_').replace('{', '_').replace('}', '_').replace('\"', '_')\n",
    "            replaced_columns.append((column_name, new_column_name))\n",
    "    return replaced_columns\n",
    "\n",
    "# Create dummies\n",
    "replaced_columns = check_and_replace_column_names(startup_df)\n",
    "\n",
    "# Print the old and new column names\n",
    "for old_name, new_name in replaced_columns:\n",
    "    print(f\"Replacing '{old_name}' with '{new_name}'\")\n",
    "\n",
    "# Replace non-compliant characters in column names\n",
    "startup_df.rename(columns=dict(replaced_columns), inplace=True)\n",
    "\n",
    "# Check the modified column names\n",
    "print(\"Modified columns:\", startup_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928a0e3-3d4a-4fb0-8435-a1f2842cf81d",
   "metadata": {},
   "source": [
    "### 2.5 Defining, tuning and evaluating model\n",
    "The below code first splits the data in test and training sets using a 70/30 training/test split. The predictors are defined by all variables besides the target variable (funding_total_usd) and the feature permalink. Then, the code defines the KNN model using a pipeline. This pipeline encompasses several essential steps: binary encoding is applied to categorical variables, missing values are imputed using the median, and numerical features are standardized. The team decided to replace the missing values with the median due to its ability to maintain the data distribution and robustness to outliers.\r\n",
    "\r\n",
    "To optimize the KNN model's performance, a random grid search is conducted. the objective is to fine-tune the hyperparameter number of neighbours, with thR²ed score serving as the guiding evaluation metric. Through 20 iterations of random hyperparameter combinations and 5-fold cross-validation, the search aims to identify the optimal configuration for the KNN model. The team used a random grid search with 20 iterations over a normal grid search as computational power and time was restricted. The random grid search still provides a basis for hyperparameter tuning. \r\n",
    "\r\n",
    "Subsequently, the best-performing model resulting from the random grid search is evaluated using the test dataset. This evaluation is based on R²ared score, providing insight into the model's predictive accuracy. The number of neighbours of the best model was found to be 3 witR²ared of 0.2618, showcasing relatively low predictive power. Only 26.18% of the total variance in the outcome variable can be explained by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73da59e8-6793-4bca-bdbb-cde243204900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: predictors (all columns except 'funding_total_usd')\n",
    "# y: target variable ('funding_total_usd')\n",
    "X = startup_df.drop(columns=['funding_total_usd', 'permalink'])\n",
    "y = startup_df['funding_total_usd']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define categorical columns for binary encoding\n",
    "categorical_columns = ['category_list', 'market', 'status', 'country_code', 'state_code', 'region',\n",
    "                      'acquirer_category_list', 'acquirer_market', 'acquirer_country_code',\n",
    "                      'acquirer_state_code', 'acquirer_region', 'funding_round_type',\n",
    "                      'investor_country_code', 'investor_state_code', 'investor_market',\n",
    "                      'investor_region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1f5dcb-f6b6-4283-ab7d-adf8968a7002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['investor_category_list']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R-squared score: 0.2618\n",
      "Best number of neighbors (k): 3\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with binary encoding, imputation, scaling, and KNN model\n",
    "pipeline = Pipeline([\n",
    "    ('binary_encoder', BinaryEncoder(cols=categorical_columns)),\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "    ('scaler', StandardScaler()),  # Standardize numerical features\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# Define hyperparameter distribution for random grid search\n",
    "param_dist = {\n",
    "    'knn__n_neighbors': randint(1, 31)\n",
    "}\n",
    "\n",
    "# Perform random grid search\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, cv=5, scoring='r2', verbose=1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_knn_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_knn_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best R-squared score: {r2:.4f}\")\n",
    "print(f\"Best number of neighbors (k): {random_search.best_params_['knn__n_neighbors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793bfd7-b1fc-464f-9e84-4e1f43b03182",
   "metadata": {},
   "source": [
    "# 3. Random Forest\n",
    "### 3.1 Model introduction\n",
    "he Random Forest algorithm was selected for its exceptional capability to manage complex datasets with a mix of numerical and categorical variables. This ensemble method, which constructs multiple decision trees and aggregates their outputs to produce a single result, is renowned for its accuracy and robustness. A major draw of Random Forest is its natural ability to handle overfitting. This is particularly advantageous when dealing with large datasets, as it maintains excellent performance without the typical risk of fitting too closely to the training data.\n",
    "\n",
    "Another appealing feature of Random Forest is its proficiency in processing both continuous and categorical data, which simplifies the preprocessing phase by minimizing the need for transforming all data into a uniform format. This model also offers interpretability through feature importance scores, allowing us to identify which variables most significantly impact predictions. This aspect is crucial for understanding the driving factors behind startup funding amounts, providing actionable insights that can guide further data collection and feature engineering efforts.\n",
    "\n",
    "However, despite its numerous advantages, Random Forest comes with certain drawbacks. It can be computationally intensive, especially with larger datasets and a high number of trees in the forest, leading to longer training times. This model also tends to perform less effectively when there are extremely noisy classification/regression tasks and unprocessed datasets with many missing values. Moreover, while it offers a good degree of interpretability compared to other more complex models, the inner workings of individual trees and their interactions within the forest can still be challenging to fully decipher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c26465-90b2-4175-bd70-481be68cf39a",
   "metadata": {},
   "source": [
    "### 3.2 Loading libraries\n",
    "To start off, various libraries were imported to facilitate data manipulation, model training, and evaluation. This setup included packages like Pandas for data operations, and Scikit-learn for model building and validation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b446938a-b5da-4893-a7d9-2bf13ce335cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6128872-c999-4738-9146-4f048c1d66b3",
   "metadata": {},
   "source": [
    "### 3.3 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c062db-7af4-4673-83b3-8ed156d7d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       permalink  Sequoia Capital  Kleiner Perkins Caufield & Byers  \\\n",
      "0              0            False                             False   \n",
      "1              1            False                             False   \n",
      "2              2            False                             False   \n",
      "3              3            False                             False   \n",
      "4              4            False                             False   \n",
      "...          ...              ...                               ...   \n",
      "28714      28714            False                             False   \n",
      "28715      28715            False                             False   \n",
      "28716      28716            False                             False   \n",
      "28717      28717            False                             False   \n",
      "28718      28718            False                             False   \n",
      "\n",
      "       New Enterprise Associates  Accel Partners  Intel Capital  \\\n",
      "0                          False           False          False   \n",
      "1                          False           False          False   \n",
      "2                          False           False          False   \n",
      "3                          False           False          False   \n",
      "4                          False           False          False   \n",
      "...                          ...             ...            ...   \n",
      "28714                      False           False          False   \n",
      "28715                      False           False          False   \n",
      "28716                      False           False          False   \n",
      "28717                      False           False          False   \n",
      "28718                      False           False           True   \n",
      "\n",
      "       Draper Fisher Jurvetson (DFJ)  First Round  SV Angel  500 Startups  \\\n",
      "0                              False         True     False         False   \n",
      "1                              False        False     False         False   \n",
      "2                              False        False     False         False   \n",
      "3                              False        False     False         False   \n",
      "4                              False        False     False         False   \n",
      "...                              ...          ...       ...           ...   \n",
      "28714                          False        False     False         False   \n",
      "28715                          False        False     False         False   \n",
      "28716                          False        False     False         False   \n",
      "28717                          False        False     False         False   \n",
      "28718                          False        False     False         False   \n",
      "\n",
      "       ...  last_founded_year  last_founded_month  last_founded_dayofyear  \\\n",
      "0      ...               2012                   6                     182   \n",
      "1      ...               2012                   8                     222   \n",
      "2      ...               2011                   4                      91   \n",
      "3      ...               2014                   9                     269   \n",
      "4      ...               2013                   5                     151   \n",
      "...    ...                ...                 ...                     ...   \n",
      "28714  ...               2013                   2                      46   \n",
      "28715  ...               2013                   1                      29   \n",
      "28716  ...               2014                   3                      83   \n",
      "28717  ...               2014                   9                     253   \n",
      "28718  ...               2013                   4                      94   \n",
      "\n",
      "       acquired_year_new  acquired_month_new  acquired_dayofyear  \\\n",
      "0                 2013.0                10.0               290.0   \n",
      "1                    NaN                 NaN                 NaN   \n",
      "2                    NaN                 NaN                 NaN   \n",
      "3                    NaN                 NaN                 NaN   \n",
      "4                    NaN                 NaN                 NaN   \n",
      "...                  ...                 ...                 ...   \n",
      "28714                NaN                 NaN                 NaN   \n",
      "28715                NaN                 NaN                 NaN   \n",
      "28716                NaN                 NaN                 NaN   \n",
      "28717                NaN                 NaN                 NaN   \n",
      "28718                NaN                 NaN                 NaN   \n",
      "\n",
      "       funded_year_new  funded_month_new  funded_dayofyear  investor_count  \n",
      "0                 2012                 6               182               6  \n",
      "1                 2012                 8               222               1  \n",
      "2                 2011                 4                91               1  \n",
      "3                 2014                 8               229               1  \n",
      "4                 2013                 5               151               1  \n",
      "...                ...               ...               ...             ...  \n",
      "28714             2013                 2                46               4  \n",
      "28715             2013                 1                29               1  \n",
      "28716             2014                 3                83               2  \n",
      "28717             2014                 9               253               1  \n",
      "28718             2013                 4                94               5  \n",
      "\n",
      "[28719 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "startup_df = pd.read_csv('cleaned.csv')\n",
    "print(startup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3e6eb-e818-46ce-861f-bf6e699f5eae",
   "metadata": {},
   "source": [
    "### 3.4 Data engineering\n",
    "The dataset was loaded, and specific date-related columns with less relevance to funding amounts were dropped to streamline the model's input features. Categorical variables were then encoded into binary formats to make them interpretable for the model. Thereafter, the preprocessed dataset underwent further cleaning to remove columns that only contained NaN values, which could otherwise skew the model's performance. Columns were also renamed to ensure consistency and avoid issues with illegal characters that might interfere with coding syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1036499-713b-4555-872b-96caca9456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified columns: Index(['permalink', 'Sequoia_Capital', 'Kleiner_Perkins_Caufield_&_Byers',\n",
      "       'New_Enterprise_Associates', 'Accel_Partners', 'Intel_Capital',\n",
      "       'Draper_Fisher_Jurvetson_(DFJ)', 'First_Round', 'SV_Angel',\n",
      "       '500_Startups',\n",
      "       ...\n",
      "       'investor_region_Winnipeg', 'investor_region_Winston-Salem',\n",
      "       'investor_region_Wollerau', 'investor_region_Worcester',\n",
      "       'investor_region_Wrexham', 'investor_region_Zagreb',\n",
      "       'investor_region_Zaragoza', 'investor_region_Zhejiang',\n",
      "       'investor_region_Zurich', 'investor_region_Çan'],\n",
      "      dtype='object', length=3207)\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['founded_at', 'first_funding_at', 'last_funding_at', 'acquired_at', 'funded_at']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "startup_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# List of categorical variables to be transformed into dummy variables\n",
    "categorical_vars = [\n",
    "    'category_list', 'market', 'status', 'country_code', 'state_code', 'region',\n",
    "    'acquirer_category_list', 'acquirer_market', 'acquirer_country_code',\n",
    "    'acquirer_state_code', 'acquirer_region', 'funding_round_type',\n",
    "    'investor_country_code', 'investor_state_code', 'investor_market',\n",
    "    'investor_region'\n",
    "]\n",
    "\n",
    "# Transform categorical variables into dummy variables (bool values)\n",
    "startup_df_dummies = pd.get_dummies(startup_df, columns=categorical_vars, drop_first=True, dtype=bool)\n",
    "startup_df_dummies.rename(columns=lambda x: x.replace(' ', '_').replace(',', '_').replace(':', '_').replace('.', '_').replace('[', '_').replace(']', '_').replace('{', '_').replace('}', '_').replace('\"', '_'), inplace=True)\n",
    "\n",
    "# Check for any completely NaN columns and drop them\n",
    "nan_columns = startup_df_dummies.columns[startup_df_dummies.isna().all()].tolist()\n",
    "startup_df_dummies.drop(columns=nan_columns, inplace=True)\n",
    "\n",
    "print(\"Modified columns:\", startup_df_dummies.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86f01d-086c-4a00-8f12-a2886c9a4601",
   "metadata": {},
   "source": [
    "### 3.5 Data splitting\n",
    "The dataset was split into 60% training, 20% validation, and 20% testing sets. This distribution allows ample data for training while providing sufficient data for both validating model tweaks and independently testing the model’s performance, mimicking realistic scenarios where the model predicts new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b92437-c73f-4e61-95fd-8b3fffeb67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"funding_total_usd\"\n",
    "\n",
    "# Split the data into train, validation, and test sets (60% train, 20% validation, 20% test)\n",
    "X = startup_df_dummies.drop(columns=[target_col, \"permalink\"])\n",
    "y = startup_df_dummies[target_col]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=1998)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1998)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dd482-355b-408e-94f4-d8f791b630ed",
   "metadata": {},
   "source": [
    "### 3.6 Model configuration\n",
    "Within a pipeline, we integrated a median imputer to handle any remaining missing values and configured the Random Forest regressor. We then set up a randomized search over hyperparameters using a 5-fold cross-validation to optimize model parameters and prevent overfitting. Key hyperparameters included:\n",
    "- n_estimators: The number of trees in the forest, ranging from 100 to 1001, to explore how model complexity affects performance.\n",
    "- max_depth: The maximum depth of each tree, limited to between 3 and 8, to control overfitting by limiting how deep the trees can grow.\n",
    "- min_samples_split and min_samples_leaf: These parameters help in defining the minimum number of samples required to split a node and the minimum number of samples a leaf node must have, respectively, further controlling the growth of trees.\n",
    "- max_features: The number of features to consider when looking for the best split, set to vary between 10% and 90% of the features, influencing the diversity of the trees in the forest.\n",
    "\n",
    "The choice of 20 iterations in the randomized search was a balance between computational efficiency and thorough exploration of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea6fa7b7-f402-4647-8990-1ebad09a1faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=1998, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                              SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                             (&#x27;random_forest&#x27;,\n",
       "                                              RandomForestRegressor(random_state=1998))]),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={&#x27;random_forest__max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC2BBEEE0&gt;,\n",
       "                                        &#x27;random_...\n",
       "                                        &#x27;random_forest__min_samples_leaf&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC2B7FB50&gt;,\n",
       "                                        &#x27;random_forest__min_samples_split&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC5D87160&gt;,\n",
       "                                        &#x27;random_forest__n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC4DA7D90&gt;},\n",
       "                   random_state=1998, scoring=&#x27;neg_mean_squared_error&#x27;,\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=1998, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                              SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                             (&#x27;random_forest&#x27;,\n",
       "                                              RandomForestRegressor(random_state=1998))]),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={&#x27;random_forest__max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC2BBEEE0&gt;,\n",
       "                                        &#x27;random_...\n",
       "                                        &#x27;random_forest__min_samples_leaf&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC2B7FB50&gt;,\n",
       "                                        &#x27;random_forest__min_samples_split&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC5D87160&gt;,\n",
       "                                        &#x27;random_forest__n_estimators&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC4DA7D90&gt;},\n",
       "                   random_state=1998, scoring=&#x27;neg_mean_squared_error&#x27;,\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                (&#x27;random_forest&#x27;, RandomForestRegressor(random_state=1998))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=1998)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=1998, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('imputer',\n",
       "                                              SimpleImputer(strategy='median')),\n",
       "                                             ('random_forest',\n",
       "                                              RandomForestRegressor(random_state=1998))]),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'random_forest__max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC2BBEEE0>,\n",
       "                                        'random_...\n",
       "                                        'random_forest__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC2B7FB50>,\n",
       "                                        'random_forest__min_samples_split': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC5D87160>,\n",
       "                                        'random_forest__n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001FBC4DA7D90>},\n",
       "                   random_state=1998, scoring='neg_mean_squared_error',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('random_forest', RandomForestRegressor(random_state=1998))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for random search\n",
    "param_dist = {\n",
    "    'random_forest__n_estimators': sp_randint(100, 1001),\n",
    "    'random_forest__max_depth': sp_randint(3, 8),\n",
    "    'random_forest__min_samples_split': sp_randint(2, 21),\n",
    "    'random_forest__min_samples_leaf': sp_randint(1, 21),\n",
    "    'random_forest__max_features': sp_uniform(0.1, 0.9)\n",
    "}\n",
    "\n",
    "# Initialize 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1998)\n",
    "\n",
    "# Perform random search with cross-validation\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, cv=kf, scoring='neg_mean_squared_error', verbose=1, random_state=1998)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ded035-7fc2-4a3b-80b2-3761a7ae9c8e",
   "metadata": {},
   "source": [
    "### 3.7 Model validation & evaluation\n",
    "The best model configuration was then applied to both the validation and test sets. The best performing Random Forest had an R² of 0.52 on the test set, showing a significant improvement when comparing to the K-NN model's 0.2618. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538b5109-9743-4a88-8727-e639c5214de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'random_forest__max_depth': 6, 'random_forest__max_features': 0.7136211298249849, 'random_forest__min_samples_leaf': 6, 'random_forest__min_samples_split': 4, 'random_forest__n_estimators': 235}\n",
      "R^2 on validation set: 0.30\n",
      "R^2 on test set: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 (coefficient of determination) on the validation and test sets\n",
    "r_squared_val = r2_score(y_val, y_pred_val)\n",
    "r_squared_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print the best hyperparameters and R^2\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "print(f\"R^2 on validation set: {r_squared_val:.2f}\")\n",
    "print(f\"R^2 on test set: {r_squared_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65703d-506e-4631-9cb0-24999f227f58",
   "metadata": {},
   "source": [
    "### 3.8 Feature importance\n",
    "Finally, we extracted and analyzed the feature importances determined by the model. This analysis helps in understanding the driving factors behind the funding predictions and was used in refining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13dd8654-0b13-4417-b032-01b09d1a80b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features by importance:\n",
      "                    Feature  Importance\n",
      "55        raised_amount_usd    0.523113\n",
      "50           funding_rounds    0.202972\n",
      "77           investor_count    0.067977\n",
      "64        founded_dayofyear    0.019108\n",
      "57               gdp_capita    0.016048\n",
      "73       acquired_dayofyear    0.015819\n",
      "53  funding_round_permalink    0.013130\n",
      "63            founded_month    0.012811\n",
      "70   last_founded_dayofyear    0.010971\n",
      "56       investor_permalink    0.008983\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from the random forest model within the pipeline\n",
    "feature_importances = best_model.named_steps['random_forest'].feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print(\"Top features by importance:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a04b6-7555-4d97-96d0-03191edb87d3",
   "metadata": {},
   "source": [
    "# 4. LightGBM\n",
    "### 4.1 Model introduction\n",
    "\n",
    "LightGBM, or Light Gradient Boosting Machine, is a gradient boosting framework that is designed for efficiency, scalability, and high performance. One of LightGBM's notable features is its gradient boosting algorithm, which sequentially trains a series of weak learners, such as decision trees, and combines them to create a robust predictive model. This approch effectively learns from previous model mistakes and continuously improves prediction accuracy. \n",
    "\n",
    "Moreover, LightGBM has a histogram-based algorithm for decision tree contruction that minimizes memory usage and accelerates the training process. This makes it very usefull for large datasets such as ours'. Another crucial factor in our the model is its ability to deal with categorical features. Therefore, there is no need for encoding which streamlines the preprocessing phase. This was especially important for our predictive model due to the great number of categorical variables in the data. The below code shows a similar workflow to the above models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b02459-782a-4fed-b809-0a5e37e3b8b2",
   "metadata": {},
   "source": [
    "### 4.2 Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f21e509-3f28-4db7-833d-149952b401eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba4962-b759-4c94-8f33-26bbba1f26b7",
   "metadata": {},
   "source": [
    "### 4.3 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a80c6f21-10fc-4221-82f9-e21e794b6441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       permalink  Sequoia Capital  Kleiner Perkins Caufield & Byers  \\\n",
      "0              0            False                             False   \n",
      "1              1            False                             False   \n",
      "2              2            False                             False   \n",
      "3              3            False                             False   \n",
      "4              4            False                             False   \n",
      "...          ...              ...                               ...   \n",
      "28714      28714            False                             False   \n",
      "28715      28715            False                             False   \n",
      "28716      28716            False                             False   \n",
      "28717      28717            False                             False   \n",
      "28718      28718            False                             False   \n",
      "\n",
      "       New Enterprise Associates  Accel Partners  Intel Capital  \\\n",
      "0                          False           False          False   \n",
      "1                          False           False          False   \n",
      "2                          False           False          False   \n",
      "3                          False           False          False   \n",
      "4                          False           False          False   \n",
      "...                          ...             ...            ...   \n",
      "28714                      False           False          False   \n",
      "28715                      False           False          False   \n",
      "28716                      False           False          False   \n",
      "28717                      False           False          False   \n",
      "28718                      False           False           True   \n",
      "\n",
      "       Draper Fisher Jurvetson (DFJ)  First Round  SV Angel  500 Startups  \\\n",
      "0                              False         True     False         False   \n",
      "1                              False        False     False         False   \n",
      "2                              False        False     False         False   \n",
      "3                              False        False     False         False   \n",
      "4                              False        False     False         False   \n",
      "...                              ...          ...       ...           ...   \n",
      "28714                          False        False     False         False   \n",
      "28715                          False        False     False         False   \n",
      "28716                          False        False     False         False   \n",
      "28717                          False        False     False         False   \n",
      "28718                          False        False     False         False   \n",
      "\n",
      "       ...  last_founded_year  last_founded_month  last_founded_dayofyear  \\\n",
      "0      ...               2012                   6                     182   \n",
      "1      ...               2012                   8                     222   \n",
      "2      ...               2011                   4                      91   \n",
      "3      ...               2014                   9                     269   \n",
      "4      ...               2013                   5                     151   \n",
      "...    ...                ...                 ...                     ...   \n",
      "28714  ...               2013                   2                      46   \n",
      "28715  ...               2013                   1                      29   \n",
      "28716  ...               2014                   3                      83   \n",
      "28717  ...               2014                   9                     253   \n",
      "28718  ...               2013                   4                      94   \n",
      "\n",
      "       acquired_year_new  acquired_month_new  acquired_dayofyear  \\\n",
      "0                 2013.0                10.0               290.0   \n",
      "1                    NaN                 NaN                 NaN   \n",
      "2                    NaN                 NaN                 NaN   \n",
      "3                    NaN                 NaN                 NaN   \n",
      "4                    NaN                 NaN                 NaN   \n",
      "...                  ...                 ...                 ...   \n",
      "28714                NaN                 NaN                 NaN   \n",
      "28715                NaN                 NaN                 NaN   \n",
      "28716                NaN                 NaN                 NaN   \n",
      "28717                NaN                 NaN                 NaN   \n",
      "28718                NaN                 NaN                 NaN   \n",
      "\n",
      "       funded_year_new  funded_month_new  funded_dayofyear  investor_count  \n",
      "0                 2012                 6               182               6  \n",
      "1                 2012                 8               222               1  \n",
      "2                 2011                 4                91               1  \n",
      "3                 2014                 8               229               1  \n",
      "4                 2013                 5               151               1  \n",
      "...                ...               ...               ...             ...  \n",
      "28714             2013                 2                46               4  \n",
      "28715             2013                 1                29               1  \n",
      "28716             2014                 3                83               2  \n",
      "28717             2014                 9               253               1  \n",
      "28718             2013                 4                94               5  \n",
      "\n",
      "[28719 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "startup_df = pd.read_csv('cleaned.csv')\n",
    "\n",
    "print(startup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a631d-4bde-4634-bfec-b285c0487c4d",
   "metadata": {},
   "source": [
    "### 4.4 Data engineering\n",
    "\n",
    "The below data engineering steps are identical to those taken for K-NN (see 2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fcd9822-2885-4c54-8b39-cc4eddec648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable reflecting the duration between the raised amount and the last funded\n",
    "startup_df['funded_at'] = pd.to_datetime(startup_df['funded_at'])\n",
    "startup_df['last_funding_at'] = pd.to_datetime(startup_df['last_funding_at'])\n",
    "\n",
    "# Calculate the funding duration in days\n",
    "startup_df['funding_duration'] = (startup_df['last_funding_at'] - startup_df['funded_at']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a46ea832-04db-414f-8340-9a7eb5494d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['founded_at', 'first_funding_at', 'last_funding_at', 'acquired_at', 'funded_at']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "startup_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# List of categorical variables to be transformed into dummy variables\n",
    "#categorical_vars = ['category_list', 'market', 'status', 'country_code', 'state_code', 'region', \n",
    "#                    'acquirer_category_list', 'acquirer_market', 'acquirer_country_code', \n",
    "#                    'acquirer_state_code', 'acquirer_region', 'funding_round_type', \n",
    "#                    'investor_country_code', 'investor_state_code', 'investor_market', \n",
    "#                    'investor_region']\n",
    "\n",
    "# Transform categorical variables into dummy variables (bool values)\n",
    "#startup_df_dummies = pd.get_dummies(startup_df, columns=categorical_vars, drop_first=True, dtype=bool)\n",
    "#print(startup_df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c004517-542a-4ddc-b79a-36cc8c20fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing 'Sequoia Capital' with 'Sequoia_Capital'\n",
      "Replacing 'Kleiner Perkins Caufield & Byers' with 'Kleiner_Perkins_Caufield_&_Byers'\n",
      "Replacing 'New Enterprise Associates' with 'New_Enterprise_Associates'\n",
      "Replacing 'Accel Partners' with 'Accel_Partners'\n",
      "Replacing 'Intel Capital' with 'Intel_Capital'\n",
      "Replacing 'Draper Fisher Jurvetson (DFJ)' with 'Draper_Fisher_Jurvetson_(DFJ)'\n",
      "Replacing 'First Round' with 'First_Round'\n",
      "Replacing 'SV Angel' with 'SV_Angel'\n",
      "Replacing '500 Startups' with '500_Startups'\n",
      "Replacing 'Bessemer Venture Partners' with 'Bessemer_Venture_Partners'\n",
      "Replacing 'Greylock Partners' with 'Greylock_Partners'\n",
      "Replacing 'Lightspeed Venture Partners' with 'Lightspeed_Venture_Partners'\n",
      "Replacing 'Andreessen Horowitz' with 'Andreessen_Horowitz'\n",
      "Replacing 'Khosla Ventures' with 'Khosla_Ventures'\n",
      "Replacing 'Index Ventures' with 'Index_Ventures'\n",
      "Replacing 'Canaan Partners' with 'Canaan_Partners'\n",
      "Replacing 'Redpoint Ventures' with 'Redpoint_Ventures'\n",
      "Replacing 'Norwest Venture Partners - NVP' with 'Norwest_Venture_Partners_-_NVP'\n",
      "Replacing 'Menlo Ventures' with 'Menlo_Ventures'\n",
      "Replacing 'General Catalyst Partners' with 'General_Catalyst_Partners'\n",
      "Replacing 'U.S. Venture Partners' with 'U_S__Venture_Partners'\n",
      "Replacing 'Sigma Partners' with 'Sigma_Partners'\n",
      "Replacing 'Battery Ventures' with 'Battery_Ventures'\n",
      "Replacing 'DAG Ventures' with 'DAG_Ventures'\n",
      "Replacing 'Foundation Capital' with 'Foundation_Capital'\n",
      "Replacing 'Google Ventures' with 'Google_Ventures'\n",
      "Replacing 'Atlas Venture' with 'Atlas_Venture'\n",
      "Replacing 'Highland Capital Partners' with 'Highland_Capital_Partners'\n",
      "Replacing 'Polaris Partners' with 'Polaris_Partners'\n",
      "Replacing 'Mayfield Fund' with 'Mayfield_Fund'\n",
      "Replacing 'Silicon Valley Bank' with 'Silicon_Valley_Bank'\n",
      "Replacing 'Y Combinator' with 'Y_Combinator'\n",
      "Replacing 'Matrix Partners' with 'Matrix_Partners'\n",
      "Replacing 'True Ventures' with 'True_Ventures'\n",
      "Replacing 'Mohr Davidow Ventures' with 'Mohr_Davidow_Ventures'\n",
      "Replacing 'North Bridge Venture Partners' with 'North_Bridge_Venture_Partners'\n",
      "Replacing 'InterWest Partners' with 'InterWest_Partners'\n",
      "Replacing 'RRE Ventures' with 'RRE_Ventures'\n",
      "Replacing 'Oak Investment Partners' with 'Oak_Investment_Partners'\n",
      "Replacing 'Felicis Ventures' with 'Felicis_Ventures'\n",
      "Replacing 'Shasta Ventures' with 'Shasta_Ventures'\n",
      "Replacing 'Start-Up Chile' with 'Start-Up_Chile'\n",
      "Replacing 'Trinity Ventures' with 'Trinity_Ventures'\n",
      "Replacing 'Goldman Sachs' with 'Goldman_Sachs'\n",
      "Replacing 'Union Square Ventures' with 'Union_Square_Ventures'\n",
      "Modified columns: Index(['permalink', 'Sequoia_Capital', 'Kleiner_Perkins_Caufield_&_Byers',\n",
      "       'New_Enterprise_Associates', 'Accel_Partners', 'Intel_Capital',\n",
      "       'Draper_Fisher_Jurvetson_(DFJ)', 'First_Round', 'SV_Angel',\n",
      "       '500_Startups', 'Bessemer_Venture_Partners', 'Greylock_Partners',\n",
      "       'Benchmark', 'Lightspeed_Venture_Partners', 'Andreessen_Horowitz',\n",
      "       'Khosla_Ventures', 'Index_Ventures', 'Canaan_Partners',\n",
      "       'Redpoint_Ventures', 'Venrock', 'Norwest_Venture_Partners_-_NVP',\n",
      "       'Menlo_Ventures', 'General_Catalyst_Partners', 'U_S__Venture_Partners',\n",
      "       'Sigma_Partners', 'Battery_Ventures', 'DAG_Ventures',\n",
      "       'Foundation_Capital', 'Google_Ventures', 'Atlas_Venture',\n",
      "       'Highland_Capital_Partners', 'Techstars', 'Polaris_Partners',\n",
      "       'Mayfield_Fund', 'Silicon_Valley_Bank', 'Y_Combinator',\n",
      "       'Matrix_Partners', 'True_Ventures', 'Mohr_Davidow_Ventures',\n",
      "       'North_Bridge_Venture_Partners', 'InterWest_Partners', 'RRE_Ventures',\n",
      "       'Oak_Investment_Partners', 'Felicis_Ventures', 'Shasta_Ventures',\n",
      "       'Start-Up_Chile', 'Trinity_Ventures', 'CRV', 'Goldman_Sachs',\n",
      "       'Union_Square_Ventures', 'DCM', 'category_list', 'market',\n",
      "       'funding_total_usd', 'status', 'country_code', 'state_code', 'region',\n",
      "       'funding_rounds', 'acquirer_permalink', 'acquirer_category_list',\n",
      "       'acquirer_market', 'acquirer_country_code', 'acquirer_state_code',\n",
      "       'acquirer_region', 'price_amount', 'funding_round_permalink',\n",
      "       'funding_round_type', 'funding_round_code', 'raised_amount_usd',\n",
      "       'investor_category_list', 'investor_country_code', 'investor_permalink',\n",
      "       'investor_state_code', 'investor_market', 'investor_region',\n",
      "       'gdp_capita', 'foreign_direct_investment', 'gdp_growth',\n",
      "       'interest_rate', 'female_participation', 'founded_year',\n",
      "       'founded_month', 'founded_dayofyear', 'first_founded_year',\n",
      "       'first_founded_month', 'first_founded_dayofyear', 'last_founded_year',\n",
      "       'last_founded_month', 'last_founded_dayofyear', 'acquired_year_new',\n",
      "       'acquired_month_new', 'acquired_dayofyear', 'funded_year_new',\n",
      "       'funded_month_new', 'funded_dayofyear', 'investor_count',\n",
      "       'funding_duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def check_and_replace_column_names(df):\n",
    "    replaced_columns = []\n",
    "    for column_name in df.columns:\n",
    "        if any(c in column_name for c in [' ', ',', ':', '.', '[', ']', '{', '}', '\"']):\n",
    "            # Replace non-compliant characters with underscores\n",
    "            new_column_name = column_name.replace(' ', '_').replace(',', '_').replace(':', '_').replace('.', '_').replace('[', '_').replace(']', '_').replace('{', '_').replace('}', '_').replace('\"', '_')\n",
    "            replaced_columns.append((column_name, new_column_name))\n",
    "    return replaced_columns\n",
    "\n",
    "# Create dummies\n",
    "replaced_columns = check_and_replace_column_names(startup_df)\n",
    "\n",
    "# Print the old and new column names\n",
    "for old_name, new_name in replaced_columns:\n",
    "    print(f\"Replacing '{old_name}' with '{new_name}'\")\n",
    "\n",
    "# Replace non-compliant characters in column names\n",
    "startup_df.rename(columns=dict(replaced_columns), inplace=True)\n",
    "\n",
    "# Check the modified column names\n",
    "print(\"Modified columns:\", startup_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a81d67-5d23-449f-9de4-0b65f7e6b9bc",
   "metadata": {},
   "source": [
    "### 4.5 Defining, tuning and evaluating model\n",
    "\n",
    "The below code segments define, tune and evaluate the LightGBM regression model. It begins by selecting predictors and splitting the data into training and testing sets, like done previously. There is one crucial difference, as LightGBM is able to deal with categorical variables, these do not have to be encoded. Only one preprocessing step is done on these variables and that is setting the datatype to category. This makes them processible in the model. \n",
    "\n",
    "After the data split and preprocessing, the hyperparameter grid was defined. It specifies potential values for key parameters lik the number of leaves, alpha, lambda L1, lambda L2 and minimum data in one leaf. Next, a random grid search is coducted with 5-fold cross-validaiton to explore the hyperparameter space. The best hyperparameters are extracted and used in the fitting of the model and finally the evaluation process. The best performing model has an R² of 0.5334 on the test set. This shows a slight improvement when comparing it to the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba9b00be-14f7-435a-b7c4-c1f20f377063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sequoia_Capital  Kleiner_Perkins_Caufield_&_Byers  \\\n",
      "20999            False                             False   \n",
      "16050            False                             False   \n",
      "22019            False                             False   \n",
      "4767             False                             False   \n",
      "25269            False                             False   \n",
      "...                ...                               ...   \n",
      "21575            False                             False   \n",
      "5390             False                             False   \n",
      "860              False                             False   \n",
      "15795            False                             False   \n",
      "23654            False                             False   \n",
      "\n",
      "       New_Enterprise_Associates  Accel_Partners  Intel_Capital  \\\n",
      "20999                      False           False          False   \n",
      "16050                      False           False          False   \n",
      "22019                      False           False          False   \n",
      "4767                       False           False          False   \n",
      "25269                      False           False          False   \n",
      "...                          ...             ...            ...   \n",
      "21575                      False           False          False   \n",
      "5390                       False           False          False   \n",
      "860                        False           False          False   \n",
      "15795                      False           False          False   \n",
      "23654                      False           False          False   \n",
      "\n",
      "       Draper_Fisher_Jurvetson_(DFJ)  First_Round  SV_Angel  500_Startups  \\\n",
      "20999                          False        False     False         False   \n",
      "16050                          False        False     False         False   \n",
      "22019                          False        False     False         False   \n",
      "4767                           False        False     False         False   \n",
      "25269                          False        False     False         False   \n",
      "...                              ...          ...       ...           ...   \n",
      "21575                          False        False     False         False   \n",
      "5390                           False        False     False         False   \n",
      "860                            False        False     False         False   \n",
      "15795                          False        False     False         False   \n",
      "23654                          False        False     False         False   \n",
      "\n",
      "       Bessemer_Venture_Partners  ...  last_founded_month  \\\n",
      "20999                      False  ...                   7   \n",
      "16050                      False  ...                   5   \n",
      "22019                      False  ...                  10   \n",
      "4767                       False  ...                   6   \n",
      "25269                      False  ...                   7   \n",
      "...                          ...  ...                 ...   \n",
      "21575                      False  ...                  10   \n",
      "5390                       False  ...                  10   \n",
      "860                        False  ...                  10   \n",
      "15795                      False  ...                   3   \n",
      "23654                      False  ...                  12   \n",
      "\n",
      "       last_founded_dayofyear  acquired_year_new  acquired_month_new  \\\n",
      "20999                     182                NaN                 NaN   \n",
      "16050                     149                NaN                 NaN   \n",
      "22019                     285             2014.0                 4.0   \n",
      "4767                      174             2008.0                10.0   \n",
      "25269                     183                NaN                 NaN   \n",
      "...                       ...                ...                 ...   \n",
      "21575                     303                NaN                 NaN   \n",
      "5390                      304                NaN                 NaN   \n",
      "860                       303                NaN                 NaN   \n",
      "15795                      61                NaN                 NaN   \n",
      "23654                     336                NaN                 NaN   \n",
      "\n",
      "       acquired_dayofyear  funded_year_new  funded_month_new  \\\n",
      "20999                 NaN             2011                 7   \n",
      "16050                 NaN             2013                 5   \n",
      "22019                98.0             2010                10   \n",
      "4767                281.0             2014                 6   \n",
      "25269                 NaN             2012                 7   \n",
      "...                   ...              ...               ...   \n",
      "21575                 NaN             2010                10   \n",
      "5390                  NaN             2009                10   \n",
      "860                   NaN             2014                 2   \n",
      "15795                 NaN             2012                 3   \n",
      "23654                 NaN             2011                 4   \n",
      "\n",
      "       funded_dayofyear  investor_count  funding_duration  \n",
      "20999               182               1                 0  \n",
      "16050               149               1                 0  \n",
      "22019               285               1                 0  \n",
      "4767                174               1                 0  \n",
      "25269               183               1                 0  \n",
      "...                 ...             ...               ...  \n",
      "21575               303               1                 0  \n",
      "5390                304               1                 0  \n",
      "860                  42               1               261  \n",
      "15795                61               1                 0  \n",
      "23654               110               4               957  \n",
      "\n",
      "[20103 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the target variable and predictors\n",
    "target_col = 'funding_total_usd'\n",
    "predictors = [col for col in startup_df.columns if col != target_col and col != \"permalink\"]\n",
    " \n",
    "# Split the data into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(startup_df[predictors], startup_df[target_col], test_size=0.3, random_state=42)\n",
    " \n",
    "# Define the categorical columns\n",
    "categorical_columns = ['category_list', 'market', 'status', 'country_code', 'state_code', 'region',\n",
    "                      'acquirer_category_list', 'acquirer_market', 'acquirer_country_code',\n",
    "                      'acquirer_state_code', 'acquirer_region', 'funding_round_type',\n",
    "                      'investor_country_code', 'investor_state_code', 'investor_market',\n",
    "                      'investor_region']\n",
    " \n",
    "# Convert categorical columns to integers (binary encoding)\n",
    "for col in categorical_columns:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cde15c1e-0907-4b43-bf0d-47ae06da7402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5626\n",
      "[LightGBM] [Info] Number of data points in the train set: 20103, number of used features: 95\n",
      "[LightGBM] [Info] Start training from score 15879290.097349\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.1 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "Best parameters: {'reg_alpha': 0.1, 'num_leaves': 600, 'min_data_in_leaf': 30, 'lambda_l2': 0, 'lambda_l1': 0}\n",
      "R-squared on test set: 0.5334\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', boosting_type='gbdt', n_jobs=-1, random_state=100)\n",
    "\n",
    "# Define the hyperparameter grid for random grid search\n",
    "param_dist = {\n",
    "    'num_leaves': [31, 600],\n",
    "    'reg_alpha': [0.1, 0.5],\n",
    "    'min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "    'lambda_l1': [0, 1, 1.5],\n",
    "    'lambda_l2': [0, 1]\n",
    "}\n",
    "\n",
    "# Perform random grid search with 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=100)\n",
    "random_search = RandomizedSearchCV(estimator=lgb_model, param_distributions=param_dist, n_iter=20, cv=kf, scoring='r2', n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_lgb_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_lgb_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"R-squared on test set: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f62108b-db6a-47dd-aef0-8db70bc6996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 most important features:\n",
      "                             Feature  Importance\n",
      "67                 raised_amount_usd        8749\n",
      "64           funding_round_permalink        3625\n",
      "56                    funding_rounds        3365\n",
      "84           first_founded_dayofyear        3163\n",
      "95                  funding_duration        3064\n",
      "70                investor_permalink        2817\n",
      "79                      founded_year        2780\n",
      "87            last_founded_dayofyear        2178\n",
      "94                    investor_count        2073\n",
      "76                        gdp_growth        1849\n",
      "78              female_participation        1723\n",
      "93                  funded_dayofyear        1649\n",
      "74                        gdp_capita        1537\n",
      "75         foreign_direct_investment        1516\n",
      "82                first_founded_year        1465\n",
      "81                 founded_dayofyear        1192\n",
      "77                     interest_rate         955\n",
      "55                            region         946\n",
      "51                            market         911\n",
      "85                 last_founded_year         747\n",
      "66                funding_round_code         601\n",
      "83               first_founded_month         555\n",
      "91                   funded_year_new         472\n",
      "80                     founded_month         460\n",
      "73                   investor_region         382\n",
      "86                last_founded_month         301\n",
      "90                acquired_dayofyear         282\n",
      "72                   investor_market         272\n",
      "50                     category_list         264\n",
      "65                funding_round_type         257\n",
      "92                  funded_month_new         221\n",
      "54                        state_code         165\n",
      "71               investor_state_code         128\n",
      "52                            status         116\n",
      "63                      price_amount         108\n",
      "88                 acquired_year_new          87\n",
      "57                acquirer_permalink          80\n",
      "53                      country_code          74\n",
      "47                     Goldman_Sachs          74\n",
      "22             U_S__Venture_Partners          66\n",
      "59                   acquirer_market          63\n",
      "69             investor_country_code          59\n",
      "17                 Redpoint_Ventures          51\n",
      "1   Kleiner_Perkins_Caufield_&_Byers          49\n",
      "10                 Greylock_Partners          45\n",
      "89                acquired_month_new          42\n",
      "14                   Khosla_Ventures          29\n",
      "2          New_Enterprise_Associates          28\n",
      "11                         Benchmark          22\n",
      "15                    Index_Ventures          22\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = best_lgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances along with their corresponding names\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance values in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top n most important features\n",
    "n = 50  # Number of top features to display\n",
    "top_features = feature_importance_df.head(n)\n",
    "print(\"Top\", n, \"most important features:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b47cf-1915-4043-b53b-31921bd4c29b",
   "metadata": {},
   "source": [
    "# 5. Lasso Regression\n",
    "\n",
    "### 5.1 Model introduction\n",
    "\n",
    "Lasso regression is particularly advantageous when dealing with datasets that have a large amount of features, as it inherently performs feature selection, reducing complexity and enhancing model interpretability by penalizing the absolute size of the coefficients. Given that our dataset contains a large quantity of features, lasso regression provides us with a handy tool to reduce the high dimensionality of our featureset. \n",
    "\n",
    "On the other hand, as opposed to more complex models, Lasso assumes a linear relationship between features and target variable, which may not adequately capture more complex, non-linear relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e2649-f317-4a73-a1fe-ba2f035eecd1",
   "metadata": {},
   "source": [
    "### 5.2 Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ae0ea0a-6354-46bc-a8a1-f4bc6e72da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import BinaryEncoder\n",
    "from scipy.stats import uniform \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb793379-1756-436e-aa5b-2a77355f35d2",
   "metadata": {},
   "source": [
    "### 5.3 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a592171-c337-420f-9d26-db68a173fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display option\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63130f7f-886c-4b3e-a8c8-c01ca522a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   permalink  Sequoia Capital  Kleiner Perkins Caufield & Byers  \\\n",
      "0          0            False                             False   \n",
      "1          1            False                             False   \n",
      "2          2            False                             False   \n",
      "3          3            False                             False   \n",
      "4          4            False                             False   \n",
      "5          5            False                             False   \n",
      "6          6            False                             False   \n",
      "7          7            False                             False   \n",
      "8          8            False                             False   \n",
      "9          9            False                             False   \n",
      "\n",
      "   New Enterprise Associates  Accel Partners  Intel Capital  \\\n",
      "0                      False           False          False   \n",
      "1                      False           False          False   \n",
      "2                      False           False          False   \n",
      "3                      False           False          False   \n",
      "4                      False           False          False   \n",
      "5                      False           False          False   \n",
      "6                      False           False          False   \n",
      "7                      False           False          False   \n",
      "8                      False           False          False   \n",
      "9                      False           False          False   \n",
      "\n",
      "   Draper Fisher Jurvetson (DFJ)  First Round  SV Angel  500 Startups  ...  \\\n",
      "0                          False         True     False         False  ...   \n",
      "1                          False        False     False         False  ...   \n",
      "2                          False        False     False         False  ...   \n",
      "3                          False        False     False         False  ...   \n",
      "4                          False        False     False         False  ...   \n",
      "5                          False        False     False         False  ...   \n",
      "6                          False        False     False         False  ...   \n",
      "7                          False        False     False         False  ...   \n",
      "8                          False        False     False         False  ...   \n",
      "9                          False        False     False         False  ...   \n",
      "\n",
      "   last_founded_year  last_founded_month  last_founded_dayofyear  \\\n",
      "0               2012                   6                     182   \n",
      "1               2012                   8                     222   \n",
      "2               2011                   4                      91   \n",
      "3               2014                   9                     269   \n",
      "4               2013                   5                     151   \n",
      "5               2013                   2                      49   \n",
      "6               2014                  10                     282   \n",
      "7               2011                  12                     362   \n",
      "8               2011                   8                     235   \n",
      "9               2011                   2                      47   \n",
      "\n",
      "   acquired_year_new  acquired_month_new  acquired_dayofyear  funded_year_new  \\\n",
      "0             2013.0                10.0               290.0             2012   \n",
      "1                NaN                 NaN                 NaN             2012   \n",
      "2                NaN                 NaN                 NaN             2011   \n",
      "3                NaN                 NaN                 NaN             2014   \n",
      "4                NaN                 NaN                 NaN             2013   \n",
      "5                NaN                 NaN                 NaN             2013   \n",
      "6                NaN                 NaN                 NaN             2014   \n",
      "7                NaN                 NaN                 NaN             2011   \n",
      "8                NaN                 NaN                 NaN             2011   \n",
      "9             2012.0                10.0               277.0             2010   \n",
      "\n",
      "   funded_month_new  funded_dayofyear  investor_count  \n",
      "0                 6               182               6  \n",
      "1                 8               222               1  \n",
      "2                 4                91               1  \n",
      "3                 8               229               1  \n",
      "4                 5               151               1  \n",
      "5                 2                49               1  \n",
      "6                10               282               1  \n",
      "7                12               362               1  \n",
      "8                 8               235               1  \n",
      "9                 1                 1              10  \n",
      "\n",
      "[10 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('cleaned.csv')\n",
    "print(df.head(10))  # Checking initial data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3148eb-0ec5-43f2-8647-28a8d436e8fe",
   "metadata": {},
   "source": [
    "### 5.4 Data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f847d-1200-4493-8395-3be731a52c4b",
   "metadata": {},
   "source": [
    "\n",
    "As we prepared our dataset for lasso regression, we faced the challenge of handling missing values, since lasso inherently cannot process incomplete data. After careful consideration, we opted to impute median values for missing numeric data, primarily timestamps. The median was chosen for its robustness against outliers and its ability to represent the central tendency without significantly altering the distribution of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5742e889-23e7-4403-b7f2-ca77b49b9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute median values\n",
    "median_timestamps = df[['funded_year_new', 'funded_month_new', 'funded_dayofyear', 'acquired_year_new', 'acquired_month_new','acquired_dayofyear']].median()\n",
    "df.update(df[['funded_year_new', 'funded_month_new', 'funded_dayofyear', 'acquired_year_new', 'acquired_month_new','acquired_dayofyear']].fillna(median_timestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e314ff-743f-4b34-b838-768995b02616",
   "metadata": {},
   "source": [
    "Additionally, we engineered a new feature, funding duration. This feature captures the time span between funding events, providing the model with crucial insights. A longer funding duration might suggest a higher amount of funds raised, offering a potentially valuable predictor for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d399f0b-8cb9-4c05-aaff-b1955f9c0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date processing\n",
    "df['funded_at'] = pd.to_datetime(df['funded_at'])\n",
    "df['last_funding_at'] = pd.to_datetime(df['last_funding_at'])\n",
    "df['funding_duration'] = (df['last_funding_at'] - df['funded_at']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44edd6e-6aac-4b93-a102-429e71915e59",
   "metadata": {},
   "source": [
    "For the missing categorical data, we employed a straightforward approach of back-filling these gaps with the label 'Unknown'. This method ensured that we retained maximum data integrity without losing too many data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "536c7af5-294b-4faa-9d65-37678aa31ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill categorical missing values\n",
    "categories = ['category_list', 'market', 'status', 'state_code', 'acquirer_category_list', 'acquirer_market', \n",
    "              'acquirer_country_code', 'acquirer_state_code', 'acquirer_region', 'investor_category_list', \n",
    "              'investor_country_code', 'investor_state_code', 'investor_market', 'investor_region']\n",
    "df[categories] = df[categories].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77169a39-db11-4d75-ac79-04184a811d94",
   "metadata": {},
   "source": [
    "Thereafter, we streamlined the dataset by dropping columns that were either already processed during the data cleaning stage or were deemed unnecessary for further analysis. This not only simplified our model but also focused the training process on the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5676aecb-4904-4fef-a070-6971ee963036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused columns\n",
    "columns_to_drop = ['founded_at', 'first_funding_at', 'last_funding_at', 'acquired_at', 'funded_at']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebc8c3-700b-41cc-bfb3-c2304c28fed4",
   "metadata": {},
   "source": [
    "\n",
    "We also undertook the task of cleaning up string columns by removing unnecessary signs and characters, which was crucial for facilitating the subsequent processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce0fdece-5d6c-49f2-906b-20ee8c58d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns to ensure clean column names\n",
    "df.columns = df.columns.str.replace(' ', '_').str.replace(r'[,:.\\[\\]{}\"]', '_', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacea25c-504d-477d-8812-ec83e18c77f1",
   "metadata": {},
   "source": [
    "Given the complexity and size of our dataset, particularly with numerous categorical variables, we faced the challenge of feature explosion while trying to maintain a manageable model size. We considered various encoding techniques, including one-hot encoding and count encoding, each with its benefits and drawbacks. One-hot encoding, while comprehensive, significantly increases the number of features, potentially leading to sparse matrices. Count encoding, on the other hand, can introduce bias if certain categories dominate.\r\n",
    "\r\n",
    "Ultimately, we decided on binary encoding for our categorical variables. This method was selected because it effectively balances the need to include all categorical features in the model while limiting the increase in feature dimensions. Binary encoding transforms categorical variables into binary columns, considerably reducing the number of additional features compared to one-hot encoding but retaining more distinct category information than count encoding. This strategic choice helped streamline our feature set without sacrificing the informational value of our categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d379ce4-a3c2-4e80-96d2-0531001b7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "encoder = BinaryEncoder(cols=categorical_columns)\n",
    "data_encoded = encoder.fit_transform(df[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bfdb452-c4f9-4fad-89b8-4e436c73effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original categorical columns and concatenate encoded ones\n",
    "df.drop(columns=categorical_columns, inplace=True)\n",
    "final_data = pd.concat([df, data_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e7212-2428-4aaa-b36b-aa02dc1e82ab",
   "metadata": {},
   "source": [
    "### 5.5 Defining, tuning and evaluating model\n",
    "We initiated the modeling process for our lasso regression by undertaking several preparatory steps. These included setting up the train/test split and performing necessary transformations to meet the specific requirements of lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df41c5c3-12aa-471b-9ce7-73300949cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and predictors\n",
    "target_col = 'funding_total_usd'\n",
    "predictors = final_data.columns.difference([target_col, 'permalink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3208bb08-9852-4e77-ad63-cfc9e62d46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_data[predictors], final_data[target_col], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00cc6f-dac5-48ae-8759-501ebdcf7c14",
   "metadata": {},
   "source": [
    "Crucial to the process was scaling the data using the StandardScaler. This step standardizes features to have zero mean and unit variance, which is vital for lasso regression because it ensures the regularization penalty is uniformly applied across all coefficients. This uniformity is essential for effective feature selection and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3227ca22-955c-48cc-a13d-00d017e7edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167cb01-485c-480e-b027-7c1054983582",
   "metadata": {},
   "source": [
    "We used Lasso-CV for our model, employing a logarithmic grid of alphas and 5-fold cross-validation. This approach optimally determines the regularization strength by exploring a wide range of values on a multiplicative scale, aiding in the selection of the best alpha. This alpha is critical as it minimizes overfitting while maximizing model performance.\n",
    "\n",
    "After optimizing the model parameters, we fit the model to the training set and proceeded to make predictions on the test set. The model’s performance was evaluated using the R² score, which at approximately 0.5854, indicated that our model was reasonably predictive. In terms of predictive accuracy, it managed to surpass more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40b2d2c1-9747-41ec-90f4-f14e0bdd8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.708950629787238e+16, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.5852538693399675e+18, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.924497993301348e+18, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3882899195730162e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5414371971372993e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5583748223968008e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5601020396606153e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5602753624479218e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5602926623472128e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5602943474984342e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5602944708928047e+19, tolerance: 9156474318783884.0\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6938804041906528e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7383015247997264e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.240092369160041e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.446513049297086e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4694420084000354e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4717867842819523e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4720221897848693e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4720456302988874e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.472047853765569e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4720479550761107e+19, tolerance: 1.3309456282847504e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8325070385452171e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8727069511137624e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.292504227015694e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5205988914303226e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5465158679639953e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.549180457812754e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5494483179111e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5494750375166075e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.549477613483656e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5494777744427446e+19, tolerance: 1.3625809260122226e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4279677464958235e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7320229223663694e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1851163666139427e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4578145301389402e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4891331266170126e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4923620264831504e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4926868626790167e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4927193227714974e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4927225054200136e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4927227594594083e+19, tolerance: 1.3440177945474088e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.592980306437018e+18, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.983155206487488e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.742644266556469e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.007066248518255e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0355284800353055e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0384203285184745e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0387103902189715e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.038739353857802e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0387421888039657e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.038742410434819e+19, tolerance: 1.0898286834446376e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.52477682332783e+18, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4462929594283794e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2113845907314377e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.500844681654609e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5330099911837917e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.536301006459242e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5366315581733663e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.536664579966786e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.536667819092452e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.536668079292472e+19, tolerance: 1.350742689552402e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.464223976080153e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5191898273406693e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.207754564906402e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.453722626847054e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4806638895102878e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4834091180984455e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4836845631487455e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4837120383868047e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4837146969924305e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4837148734902755e+19, tolerance: 1.3456155452602468e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.1307037151416156e+18, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.324032224054186e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1984421513746686e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.441689135019999e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4682064007157645e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4709090111857025e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4711802386150908e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4712072918273393e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4712099063053427e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4712100764216152e+19, tolerance: 1.3449039643997982e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3661206745910153e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6286712994112274e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0767133037338186e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3415655793215443e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.371911997230702e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3750373402150937e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.375351653051033e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3753830569234735e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3753861340378194e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.375386377704581e+19, tolerance: 1.2466141464900682e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.785461391556637e+18, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5842311993417834e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1603134972728332e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.413094227912222e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4414410710167912e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4443474803341218e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4446395198706508e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.444668676388039e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4446715153599345e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\gijsw\\anaconda3\\envs\\python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4446717219320263e+19, tolerance: 1.252891289674314e+16\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression with cross-validation\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-6, 6, 13), cv=10)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_opt = Lasso(alpha=lasso_cv.alpha_)\n",
    "lasso_opt.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = lasso_opt.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ec5151f-bfd0-4fec-a5ae-eb239ebbdb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-Score: 0.5854\n"
     ]
    }
   ],
   "source": [
    "# Calculate R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R2-Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893432a3-b970-4f3d-ad7d-db2f11ddcbce",
   "metadata": {},
   "source": [
    "### 5.6 Feature importance\n",
    "\n",
    "Finally, we analyzed the coefficients derived from the lasso model to identify which features it deemed most significant. Among the most influential were funding_rounds, total_amount_raised_USD, acquirer_state_code, and involvement from Goldman Sachs. Notably, the model selected about 36 features from an initial set of around 200, demonstrating its efficiency in feature reduction and focus on the most impactful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d41bfffc-7d0b-46b8-8388-196823e6bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Feature    Importance\n",
      "168                 raised_amount_usd  5.103829e+07\n",
      "117                    funding_rounds  1.274584e+07\n",
      "81              acquirer_state_code_0  4.778666e+06\n",
      "16                      Goldman_Sachs  3.548463e+06\n",
      "122                    investor_count  3.529623e+06\n",
      "24   Kleiner_Perkins_Caufield_&_Byers  2.541039e+06\n",
      "110                  funding_duration  2.432363e+06\n",
      "1                      Accel_Partners  1.251475e+06\n",
      "18                  Greylock_Partners  1.101136e+06\n",
      "5                           Benchmark  6.925759e+05\n",
      "30          New_Enterprise_Associates  6.065994e+05\n",
      "114              funding_round_type_1  5.987934e+05\n",
      "134                 investor_market_4  5.734611e+05\n",
      "38                    Sequoia_Capital  5.634156e+05\n",
      "31      North_Bridge_Venture_Partners  5.380657e+05\n",
      "46              U_S__Venture_Partners  5.021467e+05\n",
      "11      Draper_Fisher_Jurvetson_(DFJ)  5.011850e+05\n",
      "74                  acquirer_region_1  4.040515e+05\n",
      "77                  acquirer_region_4  3.132526e+05\n",
      "128           investor_country_code_5  1.706907e+05\n",
      "50                 acquired_dayofyear  1.678927e+05\n",
      "17                    Google_Ventures  1.153671e+05\n",
      "165                          market_8  5.210823e+04\n",
      "130                 investor_market_0  0.000000e+00\n",
      "132                 investor_market_2 -0.000000e+00\n",
      "138                investor_permalink -0.000000e+00\n",
      "125           investor_country_code_2 -0.000000e+00\n",
      "124           investor_country_code_1  0.000000e+00\n",
      "137                 investor_market_7 -0.000000e+00\n",
      "126           investor_country_code_3 -0.000000e+00\n",
      "129           investor_country_code_6  0.000000e+00\n",
      "127           investor_country_code_4  0.000000e+00\n",
      "136                 investor_market_6  0.000000e+00\n",
      "131                 investor_market_1 -0.000000e+00\n",
      "135                 investor_market_5 -0.000000e+00\n",
      "133                 investor_market_3  0.000000e+00\n",
      "94                     country_code_2  0.000000e+00\n",
      "120                     interest_rate  0.000000e+00\n",
      "123           investor_country_code_0 -0.000000e+00\n",
      "121          investor_category_list_0  0.000000e+00\n",
      "102                first_founded_year -0.000000e+00\n",
      "103         foreign_direct_investment  0.000000e+00\n",
      "104                 founded_dayofyear  0.000000e+00\n",
      "105                     founded_month  0.000000e+00\n",
      "107                  funded_dayofyear  0.000000e+00\n",
      "108                  funded_month_new  0.000000e+00\n",
      "109                   funded_year_new -0.000000e+00\n",
      "111                funding_round_code  0.000000e+00\n",
      "112           funding_round_permalink -0.000000e+00\n",
      "113              funding_round_type_0  0.000000e+00\n",
      "115              funding_round_type_2  0.000000e+00\n",
      "116              funding_round_type_3 -0.000000e+00\n",
      "118                        gdp_capita -0.000000e+00\n",
      "119                        gdp_growth  0.000000e+00\n",
      "140                 investor_region_1 -0.000000e+00\n",
      "139                 investor_region_0 -0.000000e+00\n",
      "148             investor_state_code_0 -0.000000e+00\n",
      "141                 investor_region_2 -0.000000e+00\n",
      "177                          region_8 -0.000000e+00\n",
      "169                          region_0  0.000000e+00\n",
      "170                          region_1 -0.000000e+00\n",
      "171                          region_2  0.000000e+00\n",
      "172                          region_3  0.000000e+00\n",
      "173                          region_4  0.000000e+00\n",
      "174                          region_5 -0.000000e+00\n",
      "175                          region_6  0.000000e+00\n",
      "176                          region_7  0.000000e+00\n",
      "178                          region_9 -0.000000e+00\n",
      "142                 investor_region_3 -0.000000e+00\n",
      "179                      state_code_0  0.000000e+00\n",
      "180                      state_code_1 -0.000000e+00\n",
      "181                      state_code_2 -0.000000e+00\n",
      "182                      state_code_3  0.000000e+00\n",
      "183                      state_code_4  0.000000e+00\n",
      "184                      state_code_5 -0.000000e+00\n",
      "185                          status_0  0.000000e+00\n",
      "186                          status_1  0.000000e+00\n",
      "167                      price_amount -0.000000e+00\n",
      "166                          market_9 -0.000000e+00\n",
      "164                          market_7  0.000000e+00\n",
      "163                          market_6 -0.000000e+00\n",
      "144                 investor_region_5 -0.000000e+00\n",
      "146                 investor_region_7  0.000000e+00\n",
      "147                 investor_region_8  0.000000e+00\n",
      "100           first_founded_dayofyear -0.000000e+00\n",
      "149             investor_state_code_1 -0.000000e+00\n",
      "150             investor_state_code_2 -0.000000e+00\n",
      "152             investor_state_code_4 -0.000000e+00\n",
      "153             investor_state_code_5  0.000000e+00\n",
      "154            last_founded_dayofyear  0.000000e+00\n",
      "155                last_founded_month  0.000000e+00\n",
      "156                 last_founded_year  0.000000e+00\n",
      "157                          market_0 -0.000000e+00\n",
      "158                          market_1 -0.000000e+00\n",
      "159                          market_2 -0.000000e+00\n",
      "160                          market_3  0.000000e+00\n",
      "161                          market_4  0.000000e+00\n",
      "162                          market_5  0.000000e+00\n",
      "101               first_founded_month -0.000000e+00\n",
      "187                          status_2 -0.000000e+00\n",
      "99               female_participation  0.000000e+00\n",
      "28                     Menlo_Ventures  0.000000e+00\n",
      "32     Norwest_Venture_Partners_-_NVP -0.000000e+00\n",
      "33            Oak_Investment_Partners  0.000000e+00\n",
      "34                   Polaris_Partners  0.000000e+00\n",
      "35                       RRE_Ventures -0.000000e+00\n",
      "36                  Redpoint_Ventures  0.000000e+00\n",
      "37                           SV_Angel -0.000000e+00\n",
      "39                    Shasta_Ventures -0.000000e+00\n",
      "40                     Sigma_Partners -0.000000e+00\n",
      "41                Silicon_Valley_Bank  0.000000e+00\n",
      "42                     Start-Up_Chile -0.000000e+00\n",
      "44                   Trinity_Ventures -0.000000e+00\n",
      "45                      True_Ventures -0.000000e+00\n",
      "47              Union_Square_Ventures -0.000000e+00\n",
      "48                            Venrock  0.000000e+00\n",
      "49                       Y_Combinator -0.000000e+00\n",
      "51                 acquired_month_new  0.000000e+00\n",
      "52                  acquired_year_new  0.000000e+00\n",
      "29              Mohr_Davidow_Ventures -0.000000e+00\n",
      "27                      Mayfield_Fund  0.000000e+00\n",
      "55           acquirer_category_list_2 -0.000000e+00\n",
      "26                    Matrix_Partners  0.000000e+00\n",
      "2                 Andreessen_Horowitz  0.000000e+00\n",
      "3                       Atlas_Venture -0.000000e+00\n",
      "4                    Battery_Ventures -0.000000e+00\n",
      "6           Bessemer_Venture_Partners  0.000000e+00\n",
      "7                                 CRV -0.000000e+00\n",
      "8                     Canaan_Partners  0.000000e+00\n",
      "9                        DAG_Ventures -0.000000e+00\n",
      "10                                DCM  0.000000e+00\n",
      "13                        First_Round  0.000000e+00\n",
      "14                 Foundation_Capital  0.000000e+00\n",
      "15          General_Catalyst_Partners  0.000000e+00\n",
      "19          Highland_Capital_Partners -0.000000e+00\n",
      "20                     Index_Ventures  0.000000e+00\n",
      "21                      Intel_Capital  0.000000e+00\n",
      "22                 InterWest_Partners -0.000000e+00\n",
      "23                    Khosla_Ventures  0.000000e+00\n",
      "25        Lightspeed_Venture_Partners  0.000000e+00\n",
      "54           acquirer_category_list_1  0.000000e+00\n",
      "53           acquirer_category_list_0 -0.000000e+00\n",
      "56           acquirer_category_list_3  0.000000e+00\n",
      "88                    category_list_1 -0.000000e+00\n",
      "78                  acquirer_region_5 -0.000000e+00\n",
      "79                  acquirer_region_6 -0.000000e+00\n",
      "80                  acquirer_region_7  0.000000e+00\n",
      "82              acquirer_state_code_1 -0.000000e+00\n",
      "84              acquirer_state_code_3  0.000000e+00\n",
      "85              acquirer_state_code_4  0.000000e+00\n",
      "87                    category_list_0  0.000000e+00\n",
      "89                    category_list_2 -0.000000e+00\n",
      "71                  acquirer_market_8  0.000000e+00\n",
      "90                    category_list_3  0.000000e+00\n",
      "91                    category_list_4 -0.000000e+00\n",
      "92                     country_code_0  0.000000e+00\n",
      "93                     country_code_1 -0.000000e+00\n",
      "95                     country_code_3  0.000000e+00\n",
      "96                     country_code_4  0.000000e+00\n",
      "97                     country_code_5 -0.000000e+00\n",
      "76                  acquirer_region_3  0.000000e+00\n",
      "75                  acquirer_region_2 -0.000000e+00\n",
      "70                  acquirer_market_7  0.000000e+00\n",
      "64                  acquirer_market_1  0.000000e+00\n",
      "57            acquirer_country_code_0 -0.000000e+00\n",
      "58            acquirer_country_code_1 -0.000000e+00\n",
      "59            acquirer_country_code_2 -0.000000e+00\n",
      "60            acquirer_country_code_3 -0.000000e+00\n",
      "61            acquirer_country_code_4  0.000000e+00\n",
      "62            acquirer_country_code_5 -0.000000e+00\n",
      "63                  acquirer_market_0 -0.000000e+00\n",
      "67                  acquirer_market_4  0.000000e+00\n",
      "69                  acquirer_market_6 -0.000000e+00\n",
      "66                  acquirer_market_3 -0.000000e+00\n",
      "68                  acquirer_market_5  0.000000e+00\n",
      "12                   Felicis_Ventures -9.965938e+03\n",
      "72                 acquirer_permalink -1.760572e+04\n",
      "145                 investor_region_6 -3.121180e+04\n",
      "65                  acquirer_market_2 -8.929158e+04\n",
      "83              acquirer_state_code_2 -1.028872e+05\n",
      "73                  acquirer_region_0 -1.239838e+05\n",
      "151             investor_state_code_3 -1.566868e+05\n",
      "86              acquirer_state_code_5 -1.819367e+05\n",
      "143                 investor_region_4 -1.971298e+05\n",
      "106                      founded_year -2.165431e+05\n",
      "43                          Techstars -4.339002e+05\n",
      "98                     country_code_6 -4.963754e+05\n",
      "0                        500_Startups -6.349183e+05\n"
     ]
    }
   ],
   "source": [
    "#Extracting the feature importances from the lasso model. \n",
    "coefficients = lasso_opt.coef_\n",
    "\n",
    "features = pd.DataFrame({\n",
    "    'Feature': final_data[predictors].columns,\n",
    "    'Importance': coefficients\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Printing the features sorted by importance\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d626bb78-940f-483a-bcc0-7bdfbf9e4242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used: 36\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features used:\", np.sum(lasso_opt.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9269158-243e-458b-8bde-261a7eb47a0c",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae814dc7-d0b4-460c-a1c5-96f49ff3e458",
   "metadata": {},
   "source": [
    "### 6.1 Model evaluation\n",
    "To effectively compare the performance of the various models we employed, we compiled their results into the following table:\n",
    "\n",
    "| Model             | R² score |\n",
    "|-------------------|-----------|\n",
    "| K-NN               | 0.2618    | \n",
    "| Random Forest     | 0.5200    | \n",
    "| LightGBM         | 0.5334    | \n",
    "| Lasso regression  | 0.5854    | \n",
    "\n",
    "From the data presented, it is evident that Lasso regression outperforms the other models, achieving the highest R² score of 0.5854. This indicates that Lasso Regression is the most effective model at explaining the variability in the dataset, making it our preferred choice for predicting the total funding amount that startups receive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae86d0c-8294-4ca2-bd4f-2baee9975bf6",
   "metadata": {},
   "source": [
    "### 6.2 Feature importance\n",
    "Below are the top 10 features that significantly influence the prediction of funding amounts for startups, as per the lasso regression model:\n",
    "\n",
    "| Feature                                  | Importance     |\n",
    "|------------------------------------------|----------------|\n",
    "| raised_amount_usd                        | 5.103829e+07   |\n",
    "| funding_rounds                           | 1.274584e+07   |\n",
    "| acquirer_state_code_0                    | 4.778666e+06   |\n",
    "| Goldman_Sachs                            | 3.548463e+06   |\n",
    "| investor_count                           | 3.529623e+06   |\n",
    "| Kleiner_Perkins_Caufield_&_Byers         | 2.541039e+06   |\n",
    "| funding_duration                         | 2.432363e+06   |\n",
    "| Accel_Partners                           | 1.251475e+06   |\n",
    "| Greylock_Partners                        | 1.101136e+06   |\n",
    "| Benchmark                                | 6.925759e+05   |\n",
    "\n",
    "Striking is that these features not only underscore the importance of financial metrics but also reflect the impact of prominent investors, echoing patterns observed in other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ef040-1432-4bbf-9ab8-fe8d81c655c2",
   "metadata": {},
   "source": [
    "### 6.3 Limitations\n",
    "In our comparative analysis of predictive models for estimating startup funding, Lasso Regression emerged as the most effective, achieving anR² score of 0.5854. This score reflects a robust model with substantial explanatory power. However, it's crucial to consider the limitations encountered during this analysis:\n",
    "1. Resource and Time Constraints: Our analysis was constrained by limited computing power and strict time limitations, which confined the exploration to less computationally demanding models and limited the model refinement process to only 20 iterations. This not only restricted the depth of our investigation but also potentially hindered achieving optimal model performance.\n",
    "2. Data Quality and Model Assumptions: The data's completeness, accuracy, and representativeness were significant concerns, as issues like missing data and measurement errors could lead to biased predictions. Additionally, each model came with inherent assumptions which, if unmet, adversely affect their effectiveness.\n",
    "3. Feature Selection and Hyperparameter Tuning: The selection and engineering of features, along with the methodology for hyperparameter tuning, faced limitations. We were only able to explore a fraction of the hyperparameter space, which leaves room for improvement.\n",
    "4. External factors and model suitability: Although we incorporated some external variables in the dataset, we may not have adequately accounted for significant trends that impact startup succes and funding, such as economics downturns or technological breakthroughs. Moreover, the principle that no single model provides the best solution under all circumstances was evident, underscoring the need for a tailored approach based on specific data characteristics and project requirements.\n",
    "\n",
    "These limitations highlight that while our results are promising, they are conditional. The constraints on resources, time, and data quality, coupled with limited model tuning, suggest that the current outcomes can be improved. Further research with better resources and more comprehensive model exploration could yield more optimized results, enhancing accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
